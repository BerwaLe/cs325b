{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas\n",
    "import skimage\n",
    "import shapefile\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import modules\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make TF2 work with GPU (high priority)\n",
    "\n",
    "# TODO: rescale images to workable size (low priority -- use ben's for now)\n",
    "\n",
    "# TODO: filter unwanted samples e.g. clouds, outliers, etc. (low priority)\n",
    "\n",
    "# TODO: center images via mean subtraction (low priority)\n",
    "\n",
    "# TODO: make temporary train-val-test split (medium priority)\n",
    "\n",
    "config = modules.run.load_config(\"cls\", tag=\"w5\")\n",
    "dg = modules.data.DatasetGenerator(config=config)\n",
    "dataset_kenya = dg.generate_kenya()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(config[\"batch_size\"], config[\"image_size\"], config[\"image_size\"], config[\"n_channels\"]))\n",
    "\n",
    "# TODO: load ResNet, choose loss and optimizer, build out checkpointing and tensorboard.\n",
    "\n",
    "model = tf.keras.Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import modules\n",
    "\n",
    "config = modules.run.load_config(\"cls\", tag=\"w5\")\n",
    "dg = modules.data.DatasetGenerator(config=config)\n",
    "dg._setup(\"kenya\")\n",
    "sample_filenames = dg.sample(\"kenya\", N=5000, validate=False)\n",
    "formatted_filenames = [\"{}_kenya_resized.jpg\".format(sample_filename.split(\".\")[0].split(\"_\")[1]) for sample_filename, _ in list(zip(*sample_filenames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       class                     fname\n",
      "index                                 \n",
      "4544   major   31064_kenya_resized.jpg\n",
      "4545   major  313941_kenya_resized.jpg\n",
      "4546   major  258736_kenya_resized.jpg\n",
      "4547   major   64421_kenya_resized.jpg\n",
      "4548   major  487718_kenya_resized.jpg\n",
      "4549   major   61785_kenya_resized.jpg\n",
      "4550   major  345675_kenya_resized.jpg\n",
      "4551   major     709_kenya_resized.jpg\n",
      "4552   major   20393_kenya_resized.jpg\n",
      "4553   major   47884_kenya_resized.jpg\n",
      "4555   major  144752_kenya_resized.jpg\n",
      "4556   major  487386_kenya_resized.jpg\n",
      "4557   major  224609_kenya_resized.jpg\n",
      "4558   major  235932_kenya_resized.jpg\n",
      "4560   major    6821_kenya_resized.jpg\n",
      "4561   major  493404_kenya_resized.jpg\n",
      "4563   major  439947_kenya_resized.jpg\n",
      "4564   major  455755_kenya_resized.jpg\n",
      "4565   major   43541_kenya_resized.jpg\n",
      "4566   major   42578_kenya_resized.jpg\n"
     ]
    }
   ],
   "source": [
    "ids = [fname.split(\"_\")[0] for fname in formatted_filenames]\n",
    "train_df = dg.dataframes['kenya'][dg.dataframes[\"kenya\"][\"id\"].isin(ids)][[\"class\"]]\n",
    "train_df[\"fname\"] = formatted_filenames\n",
    "train_df[\"fname\"] = formatted_filenames\n",
    "\n",
    "print(train_df[4000:4020])\n",
    "# train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for el in enumerate(train_df['fname'][1001:1021]):\n",
    "#     plt.imshow(cv2.imread('local_images/kenya/' + el[1]))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10488 validated image filenames belonging to 3 classes.\n",
      "Found 4494 validated image filenames belonging to 3 classes.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              25691136  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 3075      \n",
      "=================================================================\n",
      "Total params: 40,408,899\n",
      "Trainable params: 40,408,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/BenChoi/miniconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:103: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "327/327 [==============================] - 95s 291ms/step - loss: 1.1349 - categorical_accuracy: 0.4179 - val_loss: 1.0038 - val_categorical_accuracy: 0.4746\n",
      "Epoch 2/50\n",
      "327/327 [==============================] - 93s 286ms/step - loss: 1.0018 - categorical_accuracy: 0.4792 - val_loss: 0.9601 - val_categorical_accuracy: 0.5184\n",
      "Epoch 3/50\n",
      "327/327 [==============================] - 93s 284ms/step - loss: 0.9361 - categorical_accuracy: 0.5323 - val_loss: 0.9415 - val_categorical_accuracy: 0.5166\n",
      "Epoch 4/50\n",
      "327/327 [==============================] - 93s 283ms/step - loss: 0.8723 - categorical_accuracy: 0.5817 - val_loss: 0.8439 - val_categorical_accuracy: 0.5970\n",
      "Epoch 5/50\n",
      "327/327 [==============================] - 93s 284ms/step - loss: 0.8359 - categorical_accuracy: 0.6105 - val_loss: 0.8379 - val_categorical_accuracy: 0.6074\n",
      "Epoch 6/50\n",
      "327/327 [==============================] - 93s 284ms/step - loss: 0.8117 - categorical_accuracy: 0.6274 - val_loss: 0.8262 - val_categorical_accuracy: 0.6069\n",
      "Epoch 7/50\n",
      "327/327 [==============================] - 94s 288ms/step - loss: 0.8022 - categorical_accuracy: 0.6277 - val_loss: 0.8122 - val_categorical_accuracy: 0.6100\n",
      "Epoch 8/50\n",
      "327/327 [==============================] - 93s 284ms/step - loss: 0.7823 - categorical_accuracy: 0.6367 - val_loss: 0.8119 - val_categorical_accuracy: 0.6203\n",
      "Epoch 9/50\n",
      "327/327 [==============================] - 93s 283ms/step - loss: 0.7649 - categorical_accuracy: 0.6559 - val_loss: 0.8226 - val_categorical_accuracy: 0.6277\n",
      "Epoch 10/50\n",
      "327/327 [==============================] - 93s 283ms/step - loss: 0.7565 - categorical_accuracy: 0.6667 - val_loss: 0.7997 - val_categorical_accuracy: 0.6291\n",
      "Epoch 11/50\n",
      "327/327 [==============================] - 93s 283ms/step - loss: 0.7437 - categorical_accuracy: 0.6661 - val_loss: 0.8009 - val_categorical_accuracy: 0.6419\n",
      "Epoch 12/50\n",
      "327/327 [==============================] - 93s 284ms/step - loss: 0.7322 - categorical_accuracy: 0.6771 - val_loss: 0.7950 - val_categorical_accuracy: 0.6311\n",
      "Epoch 13/50\n",
      "327/327 [==============================] - 93s 283ms/step - loss: 0.7206 - categorical_accuracy: 0.6843 - val_loss: 0.8045 - val_categorical_accuracy: 0.6293\n",
      "Epoch 14/50\n",
      "327/327 [==============================] - 93s 283ms/step - loss: 0.7109 - categorical_accuracy: 0.6861 - val_loss: 0.7763 - val_categorical_accuracy: 0.6495\n",
      "Epoch 15/50\n",
      "105/327 [========>.....................] - ETA: 55s - loss: 0.6989 - categorical_accuracy: 0.6975"
     ]
    }
   ],
   "source": [
    "from keras import preprocessing\n",
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "import keras\n",
    "\n",
    "with tf.device('device:GPU:0'):\n",
    "    \n",
    "    checkpoints_dir = os.path.join(\"data\", config[\"base\"] + config[\"tag\"], \"checkpoints\")\n",
    "    if not os.path.isdir(checkpoints_dir):\n",
    "        os.makedirs(checkpoints_dir)\n",
    "\n",
    "    tensorboard_dir = os.path.join(\"data\", config[\"base\"] + config[\"tag\"], \"tensorboard\")\n",
    "    if not os.path.isdir(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "        tensorboard_dir, \n",
    "        write_graph=True, \n",
    "        write_images=True,\n",
    "        update_freq=config[\"batch_size\"] * config[\"tensorboard_freq\"]\n",
    "    )\n",
    "\n",
    "    checkpoints_callback = keras.callbacks.ModelCheckpoint(\n",
    "        f\"{checkpoints_dir}/model.hdf5\",\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        period=1,\n",
    "    )\n",
    "\n",
    "    input_layer = keras.layers.Input(\n",
    "        shape=(\n",
    "            config[\"image_size\"],\n",
    "            config[\"image_size\"],\n",
    "            config[\"n_channels\"])\n",
    "    )\n",
    "\n",
    "    if config[\"balance_classes\"]:\n",
    "        class_weight = sklearn.utils.class_weight.compute_class_weight(\n",
    "            \"balanced\", \n",
    "            list(range(config[\"n_classes\"])), \n",
    "            [config[\"class_enum\"][v] for v in dg.dataframes[\"kenya\"][\"class\"].values]\n",
    "        )\n",
    "    else:\n",
    "        class_weight = None\n",
    "\n",
    "    train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input, \n",
    "                                             horizontal_flip=True, \n",
    "                                             vertical_flip=True,\n",
    "    #                                          rotation_range=180,\n",
    "    #                                          brightness_range=[0.0, 2.0],\n",
    "    #                                          zoom_range=[0.8, 1.0],\n",
    "    #                                          shear_range=.2,\n",
    "    #                                          channel_shift_range=.2,\n",
    "    #                                          width_shift_range=.2,\n",
    "    #                                          height_shift_range=.2,\n",
    "    #                                          fill_mode='nearest', \n",
    "                                             validation_split=.3)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory='local_images/kenya/',\n",
    "            subset='training',\n",
    "            x_col=\"fname\",\n",
    "            y_col=\"class\",\n",
    "            target_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            shuffle=True)\n",
    "\n",
    "    val_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory='local_images/kenya/',\n",
    "            subset='validation',\n",
    "            x_col=\"fname\",\n",
    "            y_col=\"class\",\n",
    "            target_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            shuffle=True)\n",
    "\n",
    "    step_size_train = train_generator.n // train_generator.batch_size\n",
    "    step_size_val = val_generator.n // val_generator.batch_size\n",
    "\n",
    "    model = VGG16(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=input_layer,\n",
    "        classes=config[\"n_classes\"]\n",
    "    )\n",
    "    \n",
    "    model.summary(0)\n",
    "\n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(3, activation=\"softmax\")(x)\n",
    "    model_final = Model(input = model.input, output = predictions)\n",
    "\n",
    "    for layer in model_final.layers:\n",
    "            layer.trainable=True\n",
    "    \n",
    "    sgd = keras.optimizers.SGD(lr=1e-4, momentum=0.9)\n",
    "    model_final.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "    model_final.summary()\n",
    "\n",
    "    step_size_train = train_generator.n // train_generator.batch_size\n",
    "    step_size_val = val_generator.n // val_generator.batch_size\n",
    "    \n",
    "#     model_final.fit_generator(\n",
    "#             train_generator,\n",
    "#             steps_per_epoch=step_size_train,\n",
    "#             epochs=config[\"n_epochs\"],\n",
    "#             validation_data=val_generator,\n",
    "#             validation_steps=step_size_val)\n",
    "    \n",
    "    model_final.fit_generator(\n",
    "        train_generator, \n",
    "        epochs=50,\n",
    "        steps_per_epoch=step_size_train,\n",
    "        callbacks=[tensorboard_callback, checkpoints_callback], \n",
    "        validation_data=val_generator, \n",
    "        validation_steps=step_size_val,\n",
    "        class_weight=class_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.evaluate_generator(val_generator, steps=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del model_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
