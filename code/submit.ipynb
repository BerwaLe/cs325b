{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tfcle\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "from modules.run import load_config, Trainer, Metrics\n",
    "from modules.data import DataManager, processing, util\n",
    "from modules.models import pretrained_cnn, pretrained_cnn_multichannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(country, config, data_manager):\n",
    "    print(f\"Preprocess {country}...\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"+=+ RENAMING IMAGES.... +=+\")\n",
    "    processing.rename(country)\n",
    "\n",
    "    print(\"+=+ FILTERING IMAGES... +=+\")\n",
    "    processing.generate_filters(country)\n",
    "\n",
    "    print(\"+=+ DOWNCROPPING IMAGES +=+\")\n",
    "    processing.downcrop(country, config[\"image_size\"])\n",
    "\n",
    "    print(\"+=+ INIT DATA MANAGER.. +=+\")\n",
    "    data_manager._setup(country)\n",
    "\n",
    "    print(\"+=+ GENERATING MASKS... +=+\")\n",
    "    if country is \"kenya\":\n",
    "        processing.generate_masks(country, config, data_manager)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_from_config(config_file, country):\n",
    "\n",
    "    config = load_config(config_file)\n",
    "    data_manager = DataManager(config)\n",
    "\n",
    "    class_weight = None\n",
    "    train_generator = None\n",
    "    val_generator = None\n",
    "\n",
    "    if country == 'kenya':\n",
    "        train_generator, val_generator, dataframe = data_manager.generate_kenya()\n",
    "        class_weight = data_manager.class_weight(\"kenya\")\n",
    "    elif country == 'peru':\n",
    "        train_generator, val_generator, dataframe = data_manager.generate_peru()\n",
    "        class_weight = class_weight=[1.64, 1, 2]\n",
    "    \n",
    "    convnet = pretrained_cnn_multichannel(config, image_size=config[\"image_size\"], n_channels=config[\"n_channels\"])\n",
    "\n",
    "    val_steps = config[\"sample\"][\"size\"] * (config[\"validation_split\"]) // config[\"batch_size\"] + 1\n",
    "\n",
    "    labels = None\n",
    "    if config['mask'] is not None:\n",
    "        epochs = 0\n",
    "        labels = []\n",
    "        for data, label in val_generator:\n",
    "            if epochs >= val_steps:\n",
    "                break\n",
    "            labels.extend(np.argmax(label, axis=1))\n",
    "            epochs += 1\n",
    "        labels = np.array(labels)\n",
    "    trainer = Trainer(config)\n",
    "    metrics_callback = Metrics(val_generator, trainer.tensorboard_dir, labels, val_steps)\n",
    "    trainer.callbacks.append(metrics_callback)\n",
    "\n",
    "    convnet.compile(loss=trainer.loss, optimizer=trainer.optimizer, metrics=config[\"weighted_metrics\"])\n",
    "\n",
    "    convnet.fit_generator(\n",
    "        train_generator, \n",
    "        config[\"sample\"][\"size\"] * (1 - config[\"validation_split\"]) // config[\"batch_size\"] + 1,\n",
    "        epochs=config[\"n_epochs\"],\n",
    "        callbacks=trainer.callbacks,\n",
    "        validation_data=val_generator, \n",
    "        validation_steps=val_steps,\n",
    "        class_weight=class_weight,\n",
    "        use_multiprocessing=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cross_domain_from_config(config_file, country):\n",
    "\n",
    "    config = load_config(config_file)\n",
    "    data_manager = DataManager(config)\n",
    "\n",
    "    class_weight = None\n",
    "    train_generator = None\n",
    "    val_generator = None\n",
    "\n",
    "    if country == 'kenya':\n",
    "        train_generator, val_generator, dataframe = data_manager.generate_kenya()\n",
    "    elif country == 'peru':\n",
    "        train_generator, val_generator, dataframe = data_manager.generate_peru()\n",
    "    \n",
    "    convnet = pretrained_cnn_multichannel(config, image_size=config[\"image_size\"], n_channels=config[\"n_channels\"])\n",
    "    return convnet, val_generator\n",
    "\n",
    "def best_weights(directory):\n",
    "    fnames = [fname for fname in os.listdir(directory) if fname.endswith(\"hdf5\")]\n",
    "    fname = min(fnames, key=lambda fname: float(fname.split(\"-\")[-1].split(\".\")[0]))\n",
    "    return fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"preprocess\")\n",
    "\n",
    "# Initialize the DataManager with no data\n",
    "config[\"use_kenya_images\"] = False\n",
    "config[\"use_peru_images\"] = False\n",
    "data_manager = DataManager(config)\n",
    "\n",
    "preprocess(\"kenya\", config, data_manager)\n",
    "preprocess(\"peru\", config, data_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 validated image filenames belonging to 2 classes.\n",
      "Found 8 validated image filenames belonging to 2 classes.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 14s 0us/step\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (32, 2) was passed for an output of shape (None, 3) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c85175cbc79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_xception_peru_rgb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"peru\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-2c8a56488e61>\u001b[0m in \u001b[0;36mrun_experiment_from_config\u001b[0;34m(config_file, country)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/cs325b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/cs325b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs325b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m~/anaconda3/envs/cs325b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    251\u001b[0m   x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    252\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m       extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    254\u001b[0m   \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;31m# If `model._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs325b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2536\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2538\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cs325b/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[1;32m    742\u001b[0m                            \u001b[0;34m' was passed for an output of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                            \u001b[0;34m' while using as loss `'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m                            \u001b[0;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[0;31mValueError\u001b[0m: A target array with shape (32, 2) was passed for an output of shape (None, 3) while using as loss `categorical_crossentropy`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "run_experiment_from_config(\"final_xception_peru_rgb\", \"peru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception and Masking Experiments\n",
    "run_experiment_from_config(\"final_xception_kenya_rgb\", \"kenya\")\n",
    "run_experiment_from_config(\"final_xception_peru_rgb\", \"peru\")\n",
    "run_experiment_from_config(\"final_xception_kenya_masked\", \"kenya\")\n",
    "run_experiment_from_config(\"final_xception_kenya_masked-inverted\", \"kenya\")\n",
    "run_experiment_from_config(\"final_xception_kenya_two_with_mask\", \"kenya\")\n",
    "\n",
    "# ResNetV2 and Binarization Experiments\n",
    "run_experiment_from_config(\"final_resnet_kenya\", \"kenya\")\n",
    "run_experiment_from_config(\"final_resnet_kenya_balanced\", \"kenya\")\n",
    "run_experiment_from_config(\"final_resnet_kenya_balanced_major_vs_all\", \"kenya\")\n",
    "run_experiment_from_config(\"final_resnet_kenya_balanced_major_vs_minor\", \"kenya\")\n",
    "run_experiment_from_config(\"final_resnet_kenya_balanced_major_vs_twotrack\", \"kenya\")\n",
    "run_experiment_from_config(\"final_resnet_kenya_balanced_minor_vs_all\", \"kenya\")\n",
    "run_experiment_from_config(\"final_resnet_kenya_balanced_minor_vs_twotrack\", \"kenya\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_file = open(os.path.join(util.root(), \"..\", \"submission_out.txt\"), \"wt\")\n",
    "to_write = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"final_xception_peru_balanced\"\n",
    "path = os.path.join(util.root(), \"data\", name)\n",
    "model, val_gen = setup_cross_domain_from_config(name, \"kenya\")\n",
    "model.load_weights(os.path.join(path, best_weights(path)))\n",
    "val_predict = np.argmax(model.predict(val_gen), axis=1)\n",
    "\n",
    "to_write += f\"acc(peru -> kenya): {str(accuracy_score(val_gen.classes, val_predict))}\\n\"\n",
    "average = \"macro\"\n",
    "to_write += f\"f_1(peru -> kenya): {str(f1_score(val_gen.classes, val_predict, average=average))}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"final_xception_kenya_balanced\"\n",
    "path = os.path.join(util.root(), \"data\", name)\n",
    "model, val_gen = setup_cross_domain_from_config(name, \"peru\")\n",
    "model.load_weights(os.path.join(path, best_weights(path)))\n",
    "val_predict = np.argmax(model.predict(val_gen), axis=1)\n",
    "\n",
    "to_write += f\"acc(kenya -> peru): {str(accuracy_score(val_gen.classes, val_predict))}\\n\"\n",
    "average = \"macro\"\n",
    "to_write += f\"f_1(kenya -> peru): {str(f1_score(val_gen.classes, val_predict, average=average))}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write += \"For precide training and validation metrics for all experiments\"   + \\\n",
    "            \"please see the Tensorboard event files in the root/data/<config>/\" + \\\n",
    "            \"tensorboard directory.\"\n",
    "\n",
    "o_file.write(to_write)\n",
    "o_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
